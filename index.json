[{"content":"Motivation For a small Kubernetes cluster, I needed a minimal dashboard where logs from all the running applications in the Kubernetes environment can be viewed or grepped from one place.\nI thought about ELK (Elastic + Logstash + Kibana) first, but then I realized my small cluster has a config of only 2 nodes with a total 2 vCPU and 4GB RAM, which wouldn‚Äôt be a wise option for Elastic to run since it requires more resources to run smoothly. I wanted to use as few resources as possible to have the logging architecture setup.\nLet‚Äôs see if the FIG ‚û° (FluentBit + InfluxDB + Grafana) stack can do for us!\n The FIG logging architecture in Kubernetes\n  The High-Level Architecture   Fluent Bit used as the log processor which will collect all the stdout from all the pods in Kubernetes and forward them to a data storage. Fluent Bit** **is a part of the Fluentd Ecosystem but uses much much fewer resources. It creates a tiny little footprint on your system‚Äôs memory. In the Kubernetes environment, it is installed as a DaemonSet resource.\n  InfluxDB will be used as data storage so that it can hold the data forwarded by Fluent Bit. InfluxDB is a popular time-series database designed to handle high write and query loads. Since we will store lots of application logs in real-time, I think Influx is a very good option here.\n  Grafana will provide supports to create dashboards and panels where we can query over the data from InfluxDB.\n  Preparing the cluster We will be using Helm to package and deploy required applications and dependencies into the cluster. Official Helm charts for both Fluent Bit, InfluxDB and Grafana are available on GitHub or Helm Hub.\nThe default settings and config values provided by Helm charts won‚Äôt work out of the box for us. So I made a GitHub repository that contains the official Fluent Bit charts, InfluxDB charts, Grafana charts, and a few config adjustments over those. We will be using this directory to deploy all of our apps. Don‚Äôt worry I will explain below which configs were adjusted to have our FIG stack!\nüòâ Pro tip: Try to use Helm 3 as it doesn‚Äôt install tiller in your cluster. But if version 2 of Helm is an absolute necessity, look at the Tillerless Helm plugin. You can read more here about why the tiller should not be installed in a cluster.\nDeploy the FIG stack To keep our deployments separated from other running applications in Kubernetes, let‚Äôs create a namespace called ‚Äòmonitoring‚Äô or a fancy name of your choice.\nkubectl create ns monitoring Since we will be using our GitHub repo, let‚Äôs clone this to the local machine.\ngit clone https://github.com/aniskhan001/fig-stack cd fig-stack 1. Deploy \u0026amp; Configure InfluxDB helm upgrade --install influxdb influxdb -n monitoring There are not many configurations needed for InfluxDB, but the credentials and DB name. We will need to create a database to store the logs forwarded by Fluent Bit. In the influxdb/values.yml file, we provide the DB name using an env variable:\nenv: - name: INFLUXDB_DB value: \u0026#34;fluentbit\u0026#34; That‚Äôs it. We are done with InfluxDB! You can continue to step 2.\nNote: we are not creating a username and password for this demo. However, in a production environment, we should always create a username and password to ensure DB security.\nTo have authentication, in the influxdb/values.yml file set these values:\nsetDefaultUser.enabled: true setDefaultUser.user.username: my_user setDefaultUser.user.password: my_pass If the value of setDefaultUser.user.password is not given it will generate a random password for us and store it as a Kubernetes secret resource.\nAlso, the persistence is disabled for this demo so that we don‚Äôt create a pvc for this. The data won‚Äôt persist if the InfluxDB instance fails at some point.\npersistence.enabled: false Don‚Äôt forget to enable persistence in a production environment.\n2. Deploy \u0026amp; Configure Fluent Bit helm upgrade --install fluent-bit fluent-bit -n monitoring The configuration here for Fluent Bit is mainly to tell it to use InfluxDB as the backend. In fluent-bit/values.yml we define it in the ‚Äòbackend‚Äô section\nbackend: type: influx influx: host: influxdb port: 8086 database: fluentbit sequence_tag: _seq Then we will need to configure the InfluxDB output plugin in Fluent Bit‚Äôs configuration. In the fluent-bit/templates/config.yml file:\ndata: fluent-bit-service.conf: |... ... [OUTPUT] Name influxdb Match * Host {{ .Values.backend.influx.host }} Port {{ .Values.backend.influx.port }} Database {{ .Values.backend.influx.database }} Sequence_Tag {{ .Values.backend.influx.sequence_tag }} 3. Deploy \u0026amp; Configure Grafana helm upgrade --install grafana grafana -n monitoring After Grafana is installed, let‚Äôs port-forward the instance to the local machine to continue for additional settings.\n# get the grafana pod name export GF_POD_NAME=$(kubectl get pod -n monitoring -l app=grafana -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) # port forward kubectl -n monitoring port-forward $GF_POD_NAME 3000 Go to: http://localhost:3000/login\nLogin with: user = admin \u0026amp; pass = badmin\nCause I have set the values of adminPassword in grafana/values.yml for this quick demo! üò¨ This is not recommended. You can set a default password here, or just leave it blank to let Helm create a random password during installation.\nThen head over to http://localhost:3000/datasources/new to add InfluxDB as a new data source and confirm these below settings:\nURL: http://influxdb:8086/ Access: Server (Default) Database: fluentbit Pressing the ‚ÄúSave \u0026amp; Test‚Äù button should save the data source.\nAnd thus we are ready to run our queries!\nExample query and dashboard  select everything (*) from a measurement in InfluxDB\n  Using InfluxQL, you can query and grep only the things that are meaningful to your needs. The Influx Query Language is a lot similar to MySQL or other SQL queries.\n Show all logs from a particular container\n  That‚Äôs a quick setup walkthrough for a lesser-known FIG stack. Initially, I needed a minimal logging setup for viewing application logs and dived into it. While it does serve my purpose; but now, I think this setup can be used to monitor lots of things using the powerful Influx Query Language. For that, the applications should provide meaningful logs to the system.\nI hope you enjoyed the walkthrough. Please let me know any questions or suggestions if you have. Also, don‚Äôt forget to create issues or send PR to the repo if things need correction and/or enhancement. I‚Äôll be happy to review and add things if necessary.\nResources   The codebase for deployment: https://github.com/aniskhan001/fig-stack\n  Fluent Bit output plugin config: https://docs.fluentbit.io/manual/v/1.3/output/influxdb\n  Learn the InfluxDB configuration: https://docs.influxdata.com/influxdb/v1.7/administration/config/\n  How InfluxSQL works: https://docs.influxdata.com/influxdb/v1.7/query_language/spec/\n  ","permalink":"https://aniskhan001.me/tech/logging-with-fluent-bit-influxdb-fig/","summary":"Motivation For a small Kubernetes cluster, I needed a minimal dashboard where logs from all the running applications in the Kubernetes environment can be viewed or grepped from one place.\nI thought about ELK (Elastic + Logstash + Kibana) first, but then I realized my small cluster has a config of only 2 nodes with a total 2 vCPU and 4GB RAM, which wouldn‚Äôt be a wise option for Elastic to run since it requires more resources to run smoothly.","title":"Logging with Fluent Bit \u0026¬†InfluxDB"},{"content":"Motivation üåå üå† Recently I worked on a MeteorJS project. The live version of the app is hosted on Galaxy and there was no staging environment for this one. So we needed to deploy another instance of this app in a staging environment. The codebase was hosted on GitLab and we have a shared cluster of MongoDB in mLab. While we wanted to run the staging environment efficiently, we also wanted to make sure the deployment is as cost-effective as we can.\nFinding üÜì options While searching for free options, I have found these best possible options:\n Heroku is one of the most used PaaS currently which supports deploying app automatically from the GitHub repository. However, deploying from other code repositories like GitLab, Bitbucket etc aren‚Äôt supported yet.\nAt the time of this writing, they support 4 different deployment methods:\nThese are: Heroku repo (GIT), GitHub, Dropbox, and Container Registry   Heroku Deployment Methods\n   We would also need a runner for the CD to run the job that checks the successful build of the codebase. GitLab provides shared runners for one repository for free!  How I won the scenario? üèÜ Ever wished for a deploy button? Pressing that would be fun!\n the glorious deploy button\n  But in our system, we don‚Äôt even need that button to press. Here‚Äôs how:\n1. Set up Heroku API Key First, we will need an app on Heroku. If you don‚Äôt have an app yet, create one. The app name will be needed later.\nThen, we will need to set up the API key so that it can connect to the runner.\n Go to dashboard.heroku.com/account to get the Heroku API Key. Place the Heroku API Key under Settings ‚ûî CI / CD ‚ûî Variables   GitLab CI/CD Settings\n  We saved the key in a variable named HEROKU_API_KEY, we will use it later.\n2. Write the GitLab deployment script Now it‚Äôs time to write the .gitlab-ci.yml file. My deployment script goes like this:\nstaging: type: deploy script: - apt-get update -qy - apt-get install -y ruby-dev - gem install dpl - dpl --provider=heroku --app=my_app --api-key=$HEROKU_API_KEY only: - master As we can see, in the script section of the above snippet, we are installing ruby and a ruby gem called dpl. It‚Äôs a deploy tool made by Travis and can be used in GitLab as well. Then we are passing:\n provider name as heroku --provider=heroku the app name of our heroku app --app=my_app the api-key --api-key=$HEROKU_API_KEY which we have saved earlier  Finally we are defining which branch changes should trigger the deployment.\n3. Setup GitLab Runner To run the pipeline we will need to install a runner (machine) which will run scripts and other programs to make sure our app is building successfully.\nThis doc explains how we can install the runner either on our local machine or in Docker: docs.gitlab.com/runner/install\n4. Install Buildpack on Heroku (optional) for Meteor app I‚Äôd recommend using the Meteor Buildpack Horse for Meteor app with Heroku which provides easy configuration to start the app. All we need to do is to define the ROOT_URL and MONGO_URL.\n Heroku buildpack settings\n  And need to place the URL of this buildpack to Heroku, of course! Other available environment vars can be found here: github.com/AdmitHub/meteor-buildpack-horse#environment\nI hope this short guide is helpful for beginners. Experts, please let me know suggestions and/or room for improvements if there‚Äôs any. Cheers!\n","permalink":"https://aniskhan001.me/tech/easy-continuous-delivery-for-meteor-app-with-gitlab-heroku/","summary":"Motivation üåå üå† Recently I worked on a MeteorJS project. The live version of the app is hosted on Galaxy and there was no staging environment for this one. So we needed to deploy another instance of this app in a staging environment. The codebase was hosted on GitLab and we have a shared cluster of MongoDB in mLab. While we wanted to run the staging environment efficiently, we also wanted to make sure the deployment is as cost-effective as we can.","title":"Easy Continuous Delivery for Meteor app with GitLab + Heroku"},{"content":"December 2017, at work, I had to deploy a micro-service very very quickly to support the core service of ours. The framework I used for this one was Sanic, (a micro-framework written in Python 3.5 with Async support). So, I get the python image first along with dependencies in a requirements.txt file. This is how it went:\nFROMpython:3.6ENV PYTHONUNBUFFERED 1ENV TZ=Asia/DhakaRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezoneRUN mkdir /appADD requirements.txt ./app/WORKDIR/appRUN pip install -r requirements.txtCOPY ./code ./appEXPOSE5000After building this Dockerfile, the size of the image was 780 MB!\nSince I was in a hurry, (translation: I actually didn‚Äôt have good optimization knowledge in Docker, LOL) I just stared on that monstrous size for a small service and let it pass on to production. Then when I got time recently to think about optimization, gathered some knowledge from online and rewrote the file to this:\nFROMalpine:3.7RUN mkdir /appWORKDIR/appCOPY requirements.txt ./RUN apk add --no-cache python3 python3-dev build-base \u0026amp;\u0026amp; \\  python3 -m ensurepip \u0026amp;\u0026amp; \\  rm -r /usr/lib/python*/ensurepip \u0026amp;\u0026amp; \\  pip3 install --upgrade pip setuptools \u0026amp;\u0026amp; \\  pip3 install -r requirements.txt \u0026amp;\u0026amp; \\  apk del python3-dev build-base \u0026amp;\u0026amp; \\  rm -r /root/.cacheENV PYTHONUNBUFFERED 1ENV TZ=Asia/DhakaRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezoneCOPY ./code ./CMD [\u0026#34;python3\u0026#34;, \u0026#34;app.py\u0026#34;]EXPOSE5000This one looks messy, but far more optimized. After building the image with this new Dockerfile it is only 79 MB now!\nSo this is how I reduced the image size First of all, we were getting a standard python image with lots of pre-installed tools which we might not require. Instead, I‚Äôm getting an alpine based image for the second example and installing only things which are required to run this particular app.\n(extra read: is alpine worth it?)\n# nope, nope, nopeFROMpython:3.6# yeah!FROMalpine:3.7Then in the next section, notice I‚Äôm running multiple statements at once to have fewer layers. Separate RUN commands create separate layers to an image which can potentially increase the size.\n# Multiple RUN at once, for glory!RUN apk add --no-cache python3 python3-dev build-base \u0026amp;\u0026amp; \\  python3 -m ensurepip \u0026amp;\u0026amp; \\  rm -r /usr/lib/python*/ensurepip \u0026amp;\u0026amp; \\  pip3 install --upgrade pip setuptools \u0026amp;\u0026amp; \\  pip3 install -r requirements.txt \u0026amp;\u0026amp; \\  apk del python3-dev build-base \u0026amp;\u0026amp; \\  rm -r /root/.cacheBut there‚Äôs a drawback to this method as well. Once we change something to this RUN command, a rebuild would take more time since it won‚Äôt be able to take its state from the previous layer. But once again, we generally don‚Äôt add or remove packages often, so I guess this is admissible.\nLast but not least, it‚Äôs always better to clean up rm -r files and directories which aren‚Äôt required by the app. Also, be sure to add unnecessary files and directories inside .dockerignore as well so that it won‚Äôt go inside the image.\nWhy does it matter?  Less image size means less bandwidth cost. Imagine a CD pipeline that does frequent deployments where it can save more server costs Less room for security breach since we have fewer tools and dependency packages inside the image to explore Faster deployments (obviously) with small-sized image   To readers, I‚Äôm continuously learning Docker through working with it, and I found this might be an interesting story to share with everyone. Let me know your thoughts on this and share if you have any suggestions or any more tip to make Dockerfile even more optimized. Thanks for reading my story!\n","permalink":"https://aniskhan001.me/tech/how-i-reduced-the-size-of-a-docker-image/","summary":"December 2017, at work, I had to deploy a micro-service very very quickly to support the core service of ours. The framework I used for this one was Sanic, (a micro-framework written in Python 3.5 with Async support). So, I get the python image first along with dependencies in a requirements.txt file. This is how it went:\nFROMpython:3.6ENV PYTHONUNBUFFERED 1ENV TZ=Asia/DhakaRUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezoneRUN mkdir /appADD requirements.","title":"How I reduced the size of a Docker image"}]